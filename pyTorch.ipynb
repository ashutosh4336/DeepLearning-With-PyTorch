{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENV Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning With PyTorch\n"
     ]
    }
   ],
   "source": [
    "# ENV Setup\n",
    "# For Dark Theme nad to Rest to Default\n",
    "# !pip install jupyterthemes\n",
    "\n",
    "# from jupyterthemes import get_themes\n",
    "# from jupyterthemes.stylefx import set_nb_theme\n",
    "\n",
    "# set_nb_theme('onedork')\n",
    "\n",
    "# !jt -r\n",
    "%config IPCompleter.greedy=True\n",
    "print('Deep Learning With PyTorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] NOTE: Jovian is currently in beta, so if you face any issues, \n",
      "               please report them here: https://github.com/JovianML/jovian-py/issues\u001b[0m\n",
      "[jovian] Fetching aakashns/01-pytorch-basics ..\u001b[0m\n",
      "[jovian] Downloading files..\u001b[0m\n",
      "[jovian] \u001b[32mCloned successfully to '01-pytorch-basics'\u001b[0m\n",
      "\u001b[33m\u001b[4m\n",
      "Next steps:\u001b[0m\u001b[1m\n",
      "  $ cd 01-pytorch-basics\n",
      "  $ jovian install\n",
      "  $ conda activate <env_name>\n",
      "  $ jupyter notebook\n",
      "\u001b[0m\u001b[0m\n",
      "\n",
      "Replace <env_name> with the name of your environment (without the '<' & '>')\n",
      "Jovian uses Anaconda ( https://conda.io/ ) under the hood,\n",
      "so please make sure you have it installed and added to path.\n",
      "* If you face issues with `jovian install`, try `conda env update`.\n",
      "* If you face issues with `conda activate`, try `source activate <env_name>`\n",
      "  or `activate <env_name>` to activate the virtual environment.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install jovian --upgrade\n",
    "\n",
    "# !jovian clone aakashns/01-pytorch-basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we complete our discussion of tensors and gradients in PyTorch, and we're ready to move on to the next topic: *Linear regression*.\n",
    "\n",
    "## Credits\n",
    "\n",
    "The material in this series is heavily inspired by the following resources:\n",
    "\n",
    "1. [PyTorch Tutorial for Deep Learning Researchers](https://github.com/yunjey/pytorch-tutorial) by Yunjey Choi: \n",
    "\n",
    "2. [FastAI development notebooks](https://github.com/fastai/fastai_docs/tree/master/dev_nb) by Jeremy Howard: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter One [  PyTorch Basics  ]\n",
    "### Tensor and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls\n",
    "# !pwd\n",
    "# !cd 01-pytorch-basics/\n",
    "\n",
    "# !pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor(4.)\n",
    "# print(t1)\n",
    "# print(type(t1))\n",
    "# t1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "t2 = torch.tensor([1.,2,3,4])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.],\n",
       "        [10., 21.],\n",
       "        [65., 56.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix\n",
    "t3 = torch.tensor([[1.,2],[10,21],[65,56]])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11., 12., 13.],\n",
       "         [14., 15., 16.]],\n",
       "\n",
       "        [[17., 18., 19.],\n",
       "         [20., 21., 22.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-Dimension\n",
    "t4 = torch.tensor([\n",
    "    [[11,12,13],\n",
    "    [14,15,16]],\n",
    "    [[17,18,19],\n",
    "    [20.,21,22]]\n",
    "])\n",
    "# Have to supply same number of element\n",
    "# expected sequence of length 3 at dim 2 (got 4)\n",
    "\n",
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor can have any Number of Dimension, and Different lengths along each Dimension. We can inspect the length along each dimension using __.shape__ property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations and Gradients\n",
    "We can combine tensors with the usual arithmetic operations. Lets look an Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor\n",
    "x = torch.tensor(2.)\n",
    "w = torch.tensor(5., requires_grad=True)\n",
    "b = torch.tensor(9., requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've Created 3 Tensors ```x```,```w``` and ```b```, all numbers. w and b have an additional parameter __requires_grad__ set to __True__. We'll see that what it does in just a moment.\n",
    "\n",
    "Let's create a new Tensor ```y``` by combining these Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = w * x + b\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, ```y``` is a tensor with the value ```5 * 2 + 9 = 19```. What makes __PyTorch__ special is that we can automatically compute the derivative of ```y``` w.r.t. the tensors that have ```requires_grad``` set to ```True``` i.e. ```w``` and ```b```. To compute the derivatives, we can call the ```.backward``` method on our result ```y```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Derivative\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Derivative of ```y``` w.r.t the input tensors are stored in the ```.grad``` property of the respective Tesnor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(2.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display Gradients\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `dy/dw` has the same value as `x` i.e. `3`, and `dy/db` has the value `1`. Note that `x.grad` is `None`, because `x` doesn't have `requires_grad` set to `True`. \n",
    "\n",
    "The \"grad\" in `w.grad` stands for gradient, which is another term for derivative, used mainly when dealing with matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperability with ```Numpy```\n",
    "\n",
    "[Numpy](http://www.numpy.org/) is a popular open source library used for mathematical and scientific computing in Python. It enables efficient operations on large multi-dimensional arrays, and has a large ecosystem of supporting libraries:\n",
    "\n",
    "* [Matplotlib](https://matplotlib.org/) for plotting and visualization\n",
    "* [OpenCV](https://opencv.org/) for image and video processing\n",
    "* [Pandas](https://pandas.pydata.org/) for file I/O and data analysis\n",
    "\n",
    "Instead of reinventing the wheel, PyTorch interoperates really well with Numpy to leverage its existing ecosystem of tools and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]] float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = np.array([[1,2], [3,4.]])\n",
    "print(x, x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can Convert the numpy array to a torch Tensor by ```torch.from_numpy()``` or ```torch.tensor()```\n",
    "\n",
    "In ```torch.from_numpy()``` it uses same space and Memory where as ```torch.tensor()``` create a copy of the data on Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert numpy array to torch tensor:\n",
    "y = torch.from_numpy(x)\n",
    "# y = torch.tensor(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, y.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can Convert a PyTorch tensor to Numpy array using the ```.numpy``` method of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.numpy()\n",
    "print(z)\n",
    "z.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Interoperability between PyTorch and Numpy is really important because most Datasets you'll  work with will likely to be read and processed as Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jovian\n",
    "# jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter - II [  Linear Regression  ]\n",
    "\n",
    "### Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with PyTorch\n",
    "\n",
    "In  this chapter we'll discuss one of the foundational  algorithms of machine learning in this post: *Linear regression*. We'll create a model that predicts crop yields for apples and oranges (*target variables*) by looking at the average temperature, rainfall and humidity (*input variables or features*) in a region. Here's the training data:\n",
    "\n",
    "![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "```\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "```\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "![linear-regression-graph](https://i.imgur.com/4DJ9f8X.png)\n",
    "\n",
    "The *learning* part of linear regression is to figure out a set of weights `w11, w12,... w23, b1 & b2` by looking at the training data, to make accurate predictions for new data (i.e. to predict the yields for apples and oranges in a new region using the average temperature, rainfall and humidity). This is done by adjusting the weights slightly many times to make better predictions, using an optimization technique called *gradient descent*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "The training data can be represented using 2 matrices: `inputs` and `targets`, each with one row per observation, and one column per variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Data (temp, rainfall, humidity)\n",
    "inputs = np.array([\n",
    "                   [73,67,43],\n",
    "                   [91,88,64],\n",
    "                   [87,134,58],\n",
    "                   [102,43,37],\n",
    "                   [69,96,70]\n",
    "                  ], dtype='float32')\n",
    "\n",
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (Apple, Orange)\n",
    "targets = np.array([\n",
    "    [56,70],\n",
    "    [81,101],\n",
    "    [119,113],\n",
    "    [22,37],\n",
    "    [103,119]\n",
    "],dtype='float32')\n",
    "# targets.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've separated the input and target variables, because we'll operate on them separately. Also, we've created numpy arrays, because this is typically how you would work with training data: read some CSV files as numpy arrays, do some processing, and then convert them to PyTorch tensors as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 113.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model from scratch\n",
    "\n",
    "The weights and biases (`w11, w12,... w23, b1 & b2`) can also be represented as matrices, initialized as random values. The first row of `w` and the first element of `b` are used to predict the first target variable i.e. yield of apples, and similarly the second for oranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8659, -1.3779,  0.2994],\n",
      "        [ 1.6028,  0.8040, -1.5446]], requires_grad=True)\n",
      "tensor([-3.2363,  0.2313], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "\n",
    "print(w)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch.randn``` creates a tensor with given shape, with elements picked randomly from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean 0 and standard devation 1.\n",
    "\n",
    "_Our Model_ is simply a function that performs a matrix multiplication of the ```inputs``` and the weights `w`(transposed) and adds the bias `b` (replicated for each observation)  \n",
    "\n",
    "\n",
    "![matrix-mult](https://i.imgur.com/WGXLFvA.png)\n",
    "\n",
    "We can define the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-19.4703, 104.6852],\n",
      "        [-26.5326, 117.9829],\n",
      "        [-95.1784, 157.8225],\n",
      "        [ 36.9164, 141.1382],\n",
      "        [-54.8105,  79.8857]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate Prediction\n",
    "\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@` represents matrix multiplication in PyTorch, and the `.t` method returns the transpose of a tensor.\n",
    "\n",
    "The matrix obtained by passing the input data into the model is a set of predictions for the target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the predictions of our model with the actual Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 113.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# compare with target\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there's a huge difference between the predictions of our model, and the actual values of the target variables. Obviously, this is because we've initialized our model with random weights and biases, and we can't expect it to *just work*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "Before we improve our model, we need a way to evalute how well our model is performing. We can compare the model's prediction with the actual targets, using the following method.\n",
    "\n",
    "\n",
    "   * Calculate teh difference between the two matrices (preds and targets)\n",
    "   * Squre all elements of the difference matrix to remove negative values.\n",
    "   * Calculate the average of the elements in the resulting matrix.\n",
    "\n",
    "The result is a single number, known as the mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10413.3311, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = targets - preds\n",
    "diffSquare = diff * diff\n",
    "\n",
    "torch.sum(diffSquare) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squre Error (MSE) loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff *  diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.sum` returns the sum of all the elements in a tensor, and the `.numel` method returns the number of elements in a tensor. Let's compute the mean squared error for the current predictions of our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10413.3311, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
